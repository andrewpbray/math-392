<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Multiple Linear Regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Math 392" />
    <link href="libs/remark-css-0.0.1/fc.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/fc-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="reed.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Multiple Linear Regression
## The Hat Matrix and Ridge Regression
### Math 392

---


### Preamble

In the last lecture, we noted that the least squares estimate, `\(\hat{\beta} = (X'X)^{-1}X'Y\)`, is unique only if the matrix `\(X'X\)` is invertible.

Example: invert a 2 by 3 matrix.


```r
X &lt;- matrix(c(1, 1, 1, 1, 3, 5), 
            byrow = TRUE, nrow = 2)
X
```

```
##      [,1] [,2] [,3]
## [1,]    1    1    1
## [2,]    1    3    5
```

```r
solve(t(X) %*% X)
```

```
## Error in solve.default(t(X) %*% X): system is computationally singular: reciprocal condition number = 1.15648e-18
```

---

What if we transpose `\(X\)` so that it is tall rather than wide?


```r
X &lt;- t(X)
X
```

```
##      [,1] [,2]
## [1,]    1    1
## [2,]    1    3
## [3,]    1    5
```

```r
solve(t(X) %*% X)
```

```
##           [,1]   [,2]
## [1,]  1.458333 -0.375
## [2,] -0.375000  0.125
```

---
# A Very Interesting Matrix: `\(H\)`

Recall:

$$
\hat{Y} = X\hat{\beta} = X(X'X)^{-1}X'Y
$$

The "hat matrix", `\(H\)`,

$$
H = X(X'X)^{-1}X'
$$

because it puts a hat on the Y.

---
## Properties of `\(H\)`

H is *symmetric*.

`\begin{align}
H' &amp;= [X(X'X)^{-1}X']'. \\
&amp;= X'' [(X'X)^{-1}]'X' \\
&amp;= X [(X'X)']^{-1}X' \\
&amp;= X(X'X)^{-1}X' \\
&amp;= H
\end{align}`

H is *idempotent*.

`\begin{align}
H^2 &amp;= X(X'X)^{-1}X'X(X'X)^{-1}X' \\
&amp;= X(X'X)^{-1}X' \\
&amp;= H
\end{align}`

We can easily express the vector of *residuals*, `\(\hat{\epsilon}\)`.

$$
\hat{\epsilon} = Y - \hat{Y} = Y - HY = (I - H)Y
$$

---
# Leverage

Let `\(h_{i, j}\)` denote the `\((i, j)^{th}\)` element of `\(H\)`. Then we can express the fitted value of the `\(i^{th}\)` observation as

`$$\hat{Y}_i = h_{i, i}Y_i + \sum_{j \ne i}h_{i, j} Y_j$$`


---
# Leverage, cont.

In simple linear regression,

`\begin{align}
H &amp;= X(X'X)^{-1}X' \\
&amp;= 
\end{align}`

 
 
 
For the `\(i^{th}\)` observation:

$$
h_{i,i} = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{SS_X}
$$

High leverage:

`$$h_{i, i} &gt; 2 \, \textrm{avg}(h_{i,i}) = 2  \frac{p}{n}$$`


---
# Exercises

Using the hat matrix and its properties, find `\(E(\hat{\epsilon})\)` and `\(Var(\hat{\epsilon})\)`.

`\begin{align}
E(\hat{\epsilon}|X) &amp;= E((I - H)Y|X) \\
&amp;= (I - H)E(Y|X) \\
&amp;= (I - H)X\beta \\
&amp;= x\beta - X(X'X)^{-1}X'X\beta \\
&amp;= 0
\end{align}`

`\begin{align}
Var(\hat{\epsilon}|X) &amp;= Var((I - H)Y) \\
&amp;= (I - H)Var(Y)(I - H)' \\
&amp;= (I - H)\sigma^2(I - H)' \\
&amp;= \sigma^2 (II' - HI' - IH' + HH') \\
&amp;= \sigma^2 (I - H - H + H) \\
&amp;= \sigma^2(I - H)
\end{align}`


---
# Ridge Regression

Consider an alternative estimator for `\(\beta\)`.

`$$\hat{\beta}_{ridge} = \textrm{argmin}(RSS(\beta)) \quad \textrm{subject to} \quad c \ge \sum_{j = 1}^{p-1}\beta_j^2; \quad c \ge 0$$`

This is equivalent to minimizing the penalized RSS (for the scalar `\(\lambda\)`):

`\begin{align}
PRSS(\beta) &amp;= (Y - X\beta)'(Y - X\beta) + \lambda \beta'\beta \\
&amp;= Y'Y - \beta'X'Y - Y'X\beta + \beta'X'X\beta + \lambda \beta'\beta \\
&amp;= Y'Y - 2\beta'X'Y + \lambda \beta' \beta + \beta' X'X \beta.
\end{align}`

To find the `\(\hat{\beta}_{ridge}\)` that minimize this function, we take the derivative with respect to `\(\beta\)`, set to zero, and solve.

`\begin{align}
\frac{\partial \, PRSS}{\partial \, \beta} &amp;= 0 -2X'Y + 2X'X\beta + 2 \lambda \beta = 0 \\
X'Y &amp;= (X'X + \lambda I)\beta \\
\hat{\beta}_{ridge} &amp;= (X'X + \lambda I)^{-1}X'Y
\end{align}`

This approach to estimating `\(\beta\)` was originally devised as a fix for situations when `\(X'X\)` was non-invertible/singular due to multicollinearity of the predictors. 


---
# Preamble revisited

Let's apply this technique to solve the problem of invertibility that we encountered in the preamble.


```r
X &lt;- matrix(c(1, 1, 1, 1, 3, 5), byrow = TRUE, nrow = 2)
X
```

```
##      [,1] [,2] [,3]
## [1,]    1    1    1
## [2,]    1    3    5
```

```r
I &lt;- diag(ncol(X))
I
```

```
##      [,1] [,2] [,3]
## [1,]    1    0    0
## [2,]    0    1    0
## [3,]    0    0    1
```

---


```r
lambda &lt;- .5
solve(t(X) %*% X + lambda * I)
```

```
##             [,1]       [,2]        [,3]
## [1,]  1.02890173 -0.4624277  0.04624277
## [2,] -0.46242775  1.3988439 -0.73988439
## [3,]  0.04624277 -0.7398844  0.47398844
```


---
# Ridge on a tall matrix


```r
X &lt;- t(X)
X
```

```
##      [,1] [,2]
## [1,]    1    1
## [2,]    1    3
## [3,]    1    5
```

```r
I &lt;- diag(ncol(X))
I
```

```
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
```

---


```r
solve(t(X) %*% X + lambda * I)
```

```
##            [,1]        [,2]
## [1,]  0.8208092 -0.20809249
## [2,] -0.2080925  0.08092486
```

---
# Ridge on a tall matrix, cont.


```r
Y &lt;- c(3, 5, 6)
# Ridge
solve(t(X) %*% X + lambda * I) %*% t(X) %*% Y
```

```
##           [,1]
## [1,] 1.5028902
## [2,] 0.9710983
```

```r
# OLS
solve(t(X) %*% X) %*% t(X) %*% Y
```

```
##          [,1]
## [1,] 2.416667
## [2,] 0.750000
```

What's wrong?

---
# Excluding the intercept


```r
X_no_int &lt;- X[, -1]
X_no_int &lt;- X_no_int - mean(X_no_int)
I &lt;- 1
# Ridge
solve(t(X_no_int) %*% X_no_int + lambda * I) %*%
  t(X_no_int) %*% Y
```

```
##           [,1]
## [1,] 0.7058824
```

```r
# OLS
solve(t(X_no_int) %*% X_no_int) %*% t(X_no_int) %*% Y
```

```
##      [,1]
## [1,] 0.75
```


---
# Ridge vs OLS

&lt;img src="week-10-hat-matrix-slides_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---
# Properties of `\(\hat{\beta}_{ridge}\)`

`\begin{align}
E(\hat{\beta}_{ridge}|X) &amp;= E((X'X + \lambda I)^{-1}X'Y|X) \\
&amp;= (X'X + \lambda I)^{-1}X'E(Y|X) \\
&amp;= (X'X + \lambda I)^{-1}X'X\beta
\end{align}`

Therefore the ridge estimates are biased for any `\(\lambda \ne 0\)`.

`\begin{align}
Var(\hat{\beta}_{ridge}|X) &amp;= Var((X'X + \lambda I)^{-1}X'Y|X) \\
&amp;= (X'X + \lambda I)^{-1}X'Var(Y|X)[(X'X + \lambda I)^{-1}X']' \\
&amp;= (X'X + \lambda I)^{-1}X'\sigma^2 I [(X'X + \lambda I)^{-1}X']' \\
&amp;= \sigma^2 (X'X + \lambda I)^{-1} X'X (X'X + \lambda I)^{-1}
\end{align}`

Note that as `\(\lambda \rightarrow \infty\)`, `\(Var(\hat{\beta}_{ridge}) \rightarrow 0\)`.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atelier-forest-light",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
