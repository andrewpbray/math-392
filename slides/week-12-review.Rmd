---
title: "In Review: Inference for Linear Models"
author: "Math 392"
subtitle: "Problem Set Debrief"
output:
  xaringan::moon_reader:
    css: ["fc", "fc-fonts", "reed.css", "default"]
    lib_dir: libs
    nature:
      highlightStyle: atelier-forest-light
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
---

```{r include = FALSE}
knitr::opts_chunk$set(fig.align = "center", 
                      message = FALSE, 
                      warning = FALSE)

library(tidyverse)
library(rmutil)
library(mvtnorm)
library(patchwork)
set.seed(51)
```


# Inference in MLR

### 1. $Var(\hat{\beta})$

\begin{align}
Var(\hat{\beta} | X) &= Var((X'X)^{-1}X'Y | X) \\
&= (X'X)^{-1}X'Var(Y|X)X(X'X)^{-1} \\
&= (X'X)^{-1}X'\sigma^2 I X(X'X)^{-1} \\
&= \sigma^2 (X'X)^{-1}
\end{align}


---
# Inference in MLR, cont.

### 2. $Var(\hat{E}(Y|X))$

\begin{align}
Var(\hat{E}(Y|X = x_s)) &= Var(x_s\hat{\beta}) \\
&= x_s Var(\hat{\beta}) x_s' \\
&= x_s \sigma^2 (X'X)^{-1} x_s'
\end{align}


---
# Inference in MLR, cont.

### 3. $Var(Y|X = x_s))$

\begin{align}
Var(Y|X = x_s) &= Var(x_s\hat{\beta} + \epsilon) \\
&= Var(x_s\hat{\beta}) + Var(\epsilon) \\
&= x_s \sigma^2 (X'X)^{-1} x_s' + \sigma^2
\end{align}


---
# Marginal distribution of the Error

$$\epsilon \sim N(0, \sigma^2 I)$$

```{r eval = FALSE}
epsilon <- rnorm(n, mean = 0, sd = sigma_sq)
```

*Change the marginal distribution of the $\epsilon$ (though it still should be centered at 0).*

$$\epsilon \sim Unif(-1, 1)$$

```{r eval = FALSE}
epsilon <- runif(n, -1, 1)
```


$$\epsilon \sim Lap(0, b)$$

```{r eval = FALSE}
library(rmutil)
epsilon <- rlaplace(n, m = 0, s = sqrt((1/2) * sigma_sq))
```


---

```{r echo = FALSE}
B0 <- 1
B1 <- 1
B2 <- 1
B <- c(B0, B1, B2)
sigma_sq <- .2
b <- sqrt((1/2) * sigma_sq)

n <- 100
mu <- c(1, -1)
Sigma <- matrix(c(1, 0, 0, 1),
              byrow = TRUE, 
              ncol = 2)
X <- cbind(1, rmvnorm(n, mu, Sigma))
p <- ncol(X) - 1

x_s <- c(1, 1, 1)
it <- 5000
beta <- rep(NA, it)
E_y <- rep(NA, it)
y_s <- rep(NA, it)

for (j in 1:it) {
  # Simulate data
  epsilon <- rlaplace(n, m = 0, s = b)
  Y <- X %*% B + epsilon
  
  # Calculate estimates
  B_hat <- solve(t(X) %*% X) %*% t(X) %*% Y
  residuals <- Y - X %*% B_hat
  sigma_sq_hat <- sum(residuals^2) / (n - p - 1)
  beta[j] <- B_hat[2] #beta_1
  E_y[j] <- x_s %*% B_hat
  b_hat <- sqrt((1/2) * sigma_sq_hat)
  y_s[j] <- x_s %*% B_hat + rlaplace(1, m = 0, s = b_hat)
}
```

# $\hat{\beta}$

```{r echo = FALSE, fig.height = 5}
var_theory_B_hat <- sigma_sq * solve(t(X) %*% X) [2, 2]

ggplot(data.frame(beta), aes(x = beta)) +
  geom_histogram(aes(y=..density..), col = "white") +
  stat_function(fun = dnorm, 
                args = list(mean = B1, 
                            sd = sqrt(var_theory_B_hat)),
                color = "goldenrod",
                lwd = 2) +
  theme_bw()

p_norm_qq <- ggplot(data.frame(beta), aes(sample = beta)) +
  stat_qq() +
  stat_qq_line(color = "goldenrod", lwd = 2) +
  theme_bw()
```

1. Is the variance still $\sigma^2 (X'X)^{-1}$?
2. Is the distribution of $\hat{\beta}$ still normal?

---

Is the variance still $\sigma^2 (X'X)^{-1}$?

<br>
<br>
<br>
<br>
<br>
<br>

Is the distribution of $\hat{\beta}$ still normal?

---

```{r echo = FALSE, cache = FALSE}
n <- 4
X <- X[1:n, ]

for (j in 1:it) {
  # Simulate data
  epsilon <- rlaplace(n, m = 0, s = b)
  Y <- X %*% B + epsilon

  # Calculate estimates
  B_hat <- solve(t(X) %*% X) %*% t(X) %*% Y
  residuals <- Y - X %*% B_hat
  sigma_sq_hat <- sum(residuals^2) / (n - p - 1)
  beta[j] <- B_hat[2] #beta_1
  E_y[j] <- x_s %*% B_hat
  b_hat <- sqrt((1/2) * sigma_sq_hat)
  y_s[j] <- x_s %*% B_hat + rlaplace(1, m = 0, s = b_hat)
}
```

# $\hat{\beta}$, $n = 4$

```{r echo = FALSE, fig.height = 6, cache = FALSE}
var_theory_B_hat <- sigma_sq * solve(t(X) %*% X) [2, 2]

ggplot(data.frame(beta), aes(x = beta)) +
  geom_histogram(aes(y=..density..), col = "white") +
  stat_function(fun = dnorm, 
                args = list(mean = B1, 
                            sd = sqrt(var_theory_B_hat)),
                color = "goldenrod",
                lwd = 2) +
  theme_bw()
```

---
# $\hat{\beta}$, $n = 4$, cont.

```{r echo = FALSE, fig.height = 4.6, fig.width = 9}
p_notnorm_qq <- ggplot(data.frame(beta), aes(sample = beta)) +
  stat_qq() +
  stat_qq_line(color = "goldenrod", lwd = 2) +
  theme_bw()

p_notnorm_qq + p_norm_qq
```


---
# $\hat{E}(Y | X = x_s)$, $n = 4$

```{r echo = FALSE, fig.height = 6}
var_theory_E_y <- sigma_sq * t(x_s) %*% solve(t(X) %*% X) %*% x_s

ggplot(data.frame(E_y), aes(x = E_y)) +
  geom_histogram(aes(y=..density..), col = "white") +
  stat_function(fun = dnorm, 
                args = list(mean = x_s %*% B, 
                            sd = sqrt(var_theory_E_y)),
                color = "goldenrod",
                lwd = 2) +
  theme_bw()
```


---
# $Y | X = x_s$, $n = 4$

```{r echo = FALSE, fig.height = 6}
var_theory_y_s <- sigma_sq * t(x_s) %*% solve(t(X) %*% X) %*% x_s + sigma_sq

ggplot(data.frame(y_s), aes(x = y_s)) +
  geom_histogram(aes(y=..density..), col = "white") +
  stat_function(fun = dnorm, 
                args = list(mean = x_s %*% B, 
                            sd = sqrt(var_theory_y_s)),
                color = "goldenrod",
                lwd = 2) +
  theme_bw()
```


---
# Distribution of the X

$$X \sim \quad ?$$

*Introduce non-zero covariance into the joint distribution of the $X$ (`rvmnorm()` is helpful here).*

$$X \sim N\left(\mu, \sum \right)$$
$$\mu = \begin{pmatrix} -1 \\ 1 \end{pmatrix}, \quad \sum = 
\begin{pmatrix}1 & 0.5 \\ 0.5 & 1 \end{pmatrix}$$

```{r}
Sigma <- matrix(c(1, 0.5, 0.5, 1),
                byrow = TRUE, 
                ncol = 2)
X <- cbind(1, rmvnorm(n, mu, Sigma))
```

*Will this mess up the variances? The distributions?*

---
# $\hat{\beta}$, $\hat{E}(Y|X = x_s)$, $Y|X = x_s$

```{r echo = FALSE, cache = FALSE, fig.height = 4, fig.width = 10}
for (j in 1:it) {
  # Simulate data
  epsilon <- rnorm(n, mean = 0, sd = sqrt(sigma_sq))
  Y <- X %*% B + epsilon

  # Calculate estimates
  B_hat <- solve(t(X) %*% X) %*% t(X) %*% Y
  residuals <- Y - X %*% B_hat
  sigma_sq_hat <- sum(residuals^2) / (n - p - 1)
  beta[j] <- B_hat[2] #beta_1
  E_y[j] <- x_s %*% B_hat
  y_s[j] <- x_s %*% B_hat + rnorm(1, mean = 0, sd = sqrt(sigma_sq_hat))
}

var_theory_B_hat <- sigma_sq * solve(t(X) %*% X) [2, 2]
var_theory_E_y <- sigma_sq * t(x_s) %*% solve(t(X) %*% X) %*% x_s
var_theory_y_s <- sigma_sq * t(x_s) %*% solve(t(X) %*% X) %*% x_s + sigma_sq

p1 <- ggplot(data.frame(beta), aes(x = beta)) +
  geom_histogram(aes(y=..density..), col = "white") +
  stat_function(fun = dnorm, 
                args = list(mean = B1, 
                            sd = sqrt(var_theory_B_hat)),
                color = "goldenrod",
                lwd = 2) +
  theme_bw()

p2 <- ggplot(data.frame(E_y), aes(x = E_y)) +
  geom_histogram(aes(y=..density..), col = "white") +
  stat_function(fun = dnorm, 
                args = list(mean = x_s %*% B, 
                            sd = sqrt(var_theory_E_y)),
                color = "goldenrod",
                lwd = 2) +
  theme_bw()

p3 <- ggplot(data.frame(y_s), aes(x = y_s)) +
  geom_histogram(aes(y=..density..), col = "white") +
  stat_function(fun = dnorm, 
                args = list(mean = x_s %*% B, 
                            sd = sqrt(var_theory_y_s)),
                color = "goldenrod",
                lwd = 2) +
  theme_bw()

p1 + p2 + p3
```


---
# Covariance of the $\epsilon$

$$\epsilon \sim N(0, \sigma^2 I_{n \times n})$$

*Introduce non-zero covariance into the joint distribution of the $\epsilon$.*

$$\epsilon \sim N\left(\mu, \sum_{n \times n} \right)$$
$$\mu = \begin{pmatrix} -1 \\ 1 \end{pmatrix}, \quad \sum = 
\begin{pmatrix}1 & 0.5 \\ 0.5 & 1 \end{pmatrix}$$

```{r}
sigma_sq <- .2
cov <- .19
Sigma <- matrix(rep(cov, n^2), ncol = n)
diag(Sigma) <- sigma_sq
rmvnorm(1, mean = rep(0, n), sigma = Sigma)
```


---
# Covariance of the $\epsilon$

1. What distributions do you expect for the various statistics?

<br>
<br>
<br>
<br>
<br>
<br>

2. Do you expect the variances to be accurate? Underestimate? Overestimate?

---
# $\hat{\beta}$

```{r echo = FALSE, cache = FALSE, fig.height=6}
sigma_sq_hats <- rep(NA, it)
for (j in 1:it) {
  # Simulate data
  epsilon <- c(rmvnorm(1, mean = rep(0, n), sigma = Sigma))
  Y <- X %*% B + epsilon

  # Calculate estimates
  B_hat <- solve(t(X) %*% X) %*% t(X) %*% Y
  residuals <- Y - X %*% B_hat
  sigma_sq_hat <- sum(residuals^2) / (n - p - 1)
  beta[j] <- B_hat[2] #beta_1
  E_y[j] <- x_s %*% B_hat
  y_s[j] <- x_s %*% B_hat + rnorm(1, mean = 0, sd = sqrt(sigma_sq_hat))
  sigma_sq_hats[j] <- sigma_sq_hat
}

p1 <- ggplot(data.frame(beta), aes(x = beta)) +
  geom_histogram(aes(y=..density..), col = "white") +
  stat_function(fun = dnorm, 
                args = list(mean = B1, 
                            sd = sqrt(var_theory_B_hat)),
                color = "goldenrod",
                lwd = 2) +
  theme_bw()
p1
```

---
# $\hat{E}(Y | X= x_s)$

```{r echo = FALSE, fig.height=6}
p2 <- ggplot(data.frame(E_y), aes(x = E_y)) +
  geom_histogram(aes(y=..density..), col = "white") +
  stat_function(fun = dnorm, 
                args = list(mean = x_s %*% B, 
                            sd = sqrt(var_theory_E_y)),
                color = "goldenrod",
                lwd = 2) +
  theme_bw()
p2
```


---
# $Y|X=x_s$

```{r echo = FALSE, fig.height=6}
p3 <- ggplot(data.frame(y_s), aes(x = y_s)) +
  geom_histogram(aes(y=..density..), col = "white") +
  stat_function(fun = dnorm, 
                args = list(mean = x_s %*% B, 
                            sd = sqrt(var_theory_y_s)),
                color = "goldenrod",
                lwd = 2) +
  theme_bw()
p3
```

---
# Visualizing correlated errors

One draw.

```{r echo = FALSE, fig.height = 6}
X <- X[, 1:2]
B <- B[1:2]
Y <- rep(NA, 5 * 4)

for (j in 1:5) {
  epsilon <- c(rmvnorm(1, mean = rep(0, n), sigma = Sigma))
  Y[1:n + n * (j - 1)] <- X %*% B + epsilon
}

tibble(Y = Y,
       X = rep(X[, 2], 5),
       draw = factor(rep(1:5, each = n))) %>%
  filter(draw == 1) %>%
  ggplot(aes(x = X, y = Y, col = draw)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, lty = 2) +
  geom_abline(slope = 1, intercept = 1, color = "goldenrod", lwd = 2) +
  geom_vline(xintercept = X[1, 2], col = "darkgray") +
  geom_vline(xintercept = X[2, 2], col = "darkgray") +
  geom_vline(xintercept = X[3, 2], col = "darkgray") +
  geom_vline(xintercept = X[4, 2], col = "darkgray") +
  theme_bw() +
  lims(x = range(X[,2]),
       y = range(Y) + c(-.2, .2))
```


---
# Visualizing correlated errors, cont.

Two draws.

```{r echo = FALSE, fig.height = 6}
tibble(Y = Y,
       X = rep(X[, 2], 5),
       draw = factor(rep(1:5, each = n))) %>%
  filter(draw %in% 1:2) %>%
  ggplot(aes(x = X, y = Y, col = draw)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, lty = 2) +
  geom_abline(slope = 1, intercept = 1, color = "goldenrod", lwd = 2) +
  geom_vline(xintercept = X[1, 2], col = "darkgray") +
  geom_vline(xintercept = X[2, 2], col = "darkgray") +
  geom_vline(xintercept = X[3, 2], col = "darkgray") +
  geom_vline(xintercept = X[4, 2], col = "darkgray") +
  theme_bw() +
  lims(x = range(X[,2]),
       y = range(Y) + c(-.2, .2))
```

---
# Visualizing correlated errors, cont.

Five draws.

```{r echo = FALSE, fig.height = 6}
tibble(Y = Y,
       X = rep(X[, 2], 5),
       draw = factor(rep(1:5, each = n))) %>%
  ggplot(aes(x = X, y = Y, col = draw)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, lty = 2) +
  geom_abline(slope = 1, intercept = 1, color = "goldenrod", lwd = 2) +
  geom_vline(xintercept = X[1, 2], col = "darkgray") +
  geom_vline(xintercept = X[2, 2], col = "darkgray") +
  geom_vline(xintercept = X[3, 2], col = "darkgray") +
  geom_vline(xintercept = X[4, 2], col = "darkgray") +
  theme_bw() +
  lims(x = range(X[,2]),
       y = range(Y) + c(-.2, .2))
```


---
# Estimating $\sigma^2

```{r echo = FALSE, fig.height = 5, fig.width = 8}
ggplot(data.frame(s2 = sigma_sq_hats), aes(x = s2)) +
  geom_histogram(col = "white") +
  geom_vline(xintercept = sigma_sq, color = "goldenrod", lwd = 2) +
  xlim(c(-.01, .21)) +
  theme_bw()
```

We will dramatically underestimate $\sigma^2$, which goes into the SE calculations of all of our statistics.

---
# In Review: the Asymptotical Normality of the MLE

*Any MLE, $\hat{\theta}^{MLE}$ will be normally distributed as $n \rightarrow \infty$ with expected value $\theta$ and standard deviation $\frac{1}{\sqrt(nI(\theta))}$.*


---
# Example: $\hat{\beta}^{OLS}$

```{r}
m1 <- lm(mpg ~ disp + hp + wt, data = mtcars)
summary(m1)
```


---
# Example: $\hat{\beta}^{OLS}$, cont.

---
# Example: $\hat{\beta}^{Log}$

```{r}
m2 <- glm(factor(am) ~ disp + hp + wt, data = mtcars, family = "binomial")
summary(m2)
```

---
# Example: $\hat{\beta}^{Log}$, cont.
