---
title: "Gibbs Sampling"
author: "Math 392"
subtitle: "Case Study: The Changepoint Problem"
output:
  xaringan::moon_reader:
    css: ["fc", "fc-fonts", "reed.css", "default"]
    lib_dir: libs
    nature:
      highlightStyle: atelier-forest-light
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
---

```{r include = FALSE}
knitr::opts_chunk$set(message = FALSE, fig.align = "center")

library(tidyverse)
```

# For later use:

The random variable $X$ is *Poisson* if,

$$
f(x \,|\, \lambda) = \frac{\lambda^x e^{-\lambda}}{x!} \, ; \quad \quad \lambda > 0, x \in \{0, 1, 2, \ldots \}
$$

The random variable $X$ is *Gamma* if,

$$
f(x\,|\,\alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha - 1}e^{-\beta x} \, ; \quad \quad \alpha > 0, \beta > 0, x \in \{0, \infty \}
$$

where $E(X) = \frac{\alpha}{\beta}$ and $Var(X) = \frac{\alpha}{\beta^2}$.


---
# Markov Chain Monte Carlo

A collection of algorithms that allow you to take random (Monte Carlo) samples from distributions using a Markov chain.


---
# Gibbs Sampling

Suppose you wish to sample from a joint pdf with density $f(x, y)$, but are unable to do it directly (there is no `r___()` function). If we can generate from the respective *conditional* distributions, we can set up a Markov chain.

Start with a reasonable $Y_0$, then:

1. Generate $X_{i+1} \sim f_x(X | Y_i)$
2. Generate $Y_{i+1} \sim f_y(Y | X_{i+1})$

Iterate these steps, and if conditions are met, the stationary distribution will be $f(x, y)$.

Practical details:

- Use a burn-in period to escape $Y_0$.
- Take every tenth sample to diminish Markov dependence.


---
# The Changepoint Problem

Consider a series of Poisson random variables, $Y_1, Y_2, \ldots, Y_n$. At some unknown point, $m$, the rate of the Poisson distribution changed. That is,

\begin{align}
Y_1, \ldots, Y_m &\sim \textrm{Poi}(\mu) \\
Y_{m+1}, \ldots, Y_n &\sim \textrm{Poi}(\lambda)
\end{align}

```{r echo = FALSE}
n <- 60
m <- 38
mu <- 2
lambda <- 4
```

Say: $\mu =$ `r mu`,  $\lambda =$ `r lambda` and $m =$ `r m` (at $n =$ `r n`).

```{r echo = FALSE, fig.height=3}
set.seed(47)
y_mu <- rpois(m, lambda = mu)
y_lambda <- rpois(n - m, lambda = lambda)
y <- c(y_mu, y_lambda)
df <- data.frame(t = 1:n,
                 y = y,
                 rate = rep(c("mu", "lambda"), c(m, n - m)))
ggplot(df, aes(x = t, y = y)) +
  geom_point() + geom_line() +
  theme_bw() + labs(x = "index")
```

**Goal**: Estimate $m$ using data and prior information. 


---
# Plan of Attack

I. Specify prior

II. Write down full joint posterior

III. Write down joint posterior of parameters

IV. Write down conditional posteriors

V. Run Gibbs Sampler to draw from joint posterior then assess convergence and Markov dependence

VI. Formulate conclusions

---
# I. Specifying Priors

\begin{align}
\mu &\sim \textrm{Gamma}(\alpha = 10, \beta = 4) \\
\lambda &\sim \textrm{Gamma}(\nu = 8, \phi = 2).
\end{align}

```{r echo = FALSE, fig.height=5.5}
library(gridExtra)
alpha <- 10
beta <- 4
p1 <- ggplot(data.frame(x = c(0, 10)), aes(x)) +
  stat_function(fun = dgamma, 
                args = list(shape = alpha, rate = beta)) +
  labs(x = expression(mu), y = "") +
  theme_bw() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
nu <- 8
phi <- 2
p2 <- ggplot(data.frame(x = c(0, 10)), aes(x)) +
  stat_function(fun = dgamma, 
                args = list(shape = nu, rate = phi)) +
  labs(x = expression(lambda), y = "") +
  theme_bw() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
grid.arrange(p1, p2, ncol = 1)
```

---
# I. Specifying Priors, cont.

$$m \sim \textrm{Unif}\{1, 2, \ldots, n - 1\}$$

```{r echo = FALSE, fig.height=4, fig.width = 9}
ggplot(data.frame(x = 1:(n-1),
                  y = rep(1/(n-1), n - 1)), aes(x, y)) +
  geom_col() +
  labs(x = "m", y = "") +
  theme_bw() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```


---
# II. Full Joint Distribution

If they were all independent:

$$f(Y_\mu, Y_\lambda, \mu, \lambda, m) = f(Y_\mu)f(Y_\lambda)f(\mu)f(\lambda)f(m)$$

Correct joint distribution:

\begin{align}
 &= f(Y_\mu\,|\,\mu, m)f(Y_\lambda\,|\,\lambda, m)f(\mu)f(\lambda)f(m) \\
 &= 
\prod_{i = 1}^{m} \left( \frac{\mu^{Y_i}e^{-\mu}}{Y_i!} \right)
\prod_{j = m+1}^{n} \left( \frac{\lambda^{Y_j}e^{-\lambda}}{Y_j!} \right)
\frac{\beta^\alpha}{\Gamma(\alpha)}\mu^{\alpha - 1}e^{-\beta \mu}
\frac{\phi^\nu}{\Gamma(\nu)}\lambda^{\nu - 1}e^{-\phi \lambda}
\frac{1}{n - 1}
\end{align}


---
# III. Joint Posterior of Parameters

From the definition of conditional probability:

\begin{align}
f(\mu, \lambda, m \,|\, Y_\mu, Y_\lambda) &= \frac{f(Y_\mu, Y_\lambda, \mu, \lambda, m)}{f(Y_\mu, Y_\lambda)}
\end{align}

We can then pull out the terms, $c$, that are not a function of the parameters.

\begin{align}
&= c \,f(Y_\mu, Y_\lambda, \mu, \lambda, m) \\
&= c \,\prod_{i = 1}^{m} \left( \frac{\mu^{Y_i}e^{-\mu}}{Y_i!} \right)
\prod_{j = m+1}^{n} \left( \frac{\lambda^{Y_j}e^{-\lambda}}{Y_j!} \right)
\frac{\beta^\alpha}{\Gamma(\alpha)}\mu^{\alpha - 1}e^{-\beta \mu}
\frac{\phi^\nu}{\Gamma(\nu)}\lambda^{\nu - 1}e^{-\phi \lambda} \frac{1}{n - 1} \\
&= c \,\prod_{i = 1}^{m} \left( \mu^{Y_i}e^{-\mu} \right)
\prod_{j = m+1}^{n} \left( \lambda^{Y_j}e^{-\lambda} \right)
\mu^{\alpha - 1}e^{-\beta \mu}
\lambda^{\nu - 1}e^{-\phi \lambda} \\
&= c \,  \mu^{\sum_{i=1}^{m}Y_i}e^{-m\mu} 
\lambda^{\sum_{j=m+1}^{n}Y_j}e^{-(n - m)\lambda}
\mu^{\alpha - 1}e^{-\beta \mu}
\lambda^{\nu - 1}e^{-\phi \lambda} \\
&= c \,  \mu^{\alpha + \sum_{i=1}^{m}Y_i - 1}e^{-(\beta + m) \mu} 
\lambda^{\nu + \sum_{j=m+1}^{n}Y_j - 1}e^{-(\phi + n - m)\lambda}
\end{align}


---
# IV. Conditional Posterior for $m$

We again use the definition of conditional probability:

\begin{align}
f(m \,|\, \mu, \lambda, Y_\mu, Y_\lambda) &= \frac{f(\mu, \lambda, m \,|\, Y_\mu, Y_\lambda)}{f(\mu, \lambda \,|\, Y_\mu, Y_\lambda)} \\
&= \frac{c \,  \mu^{\alpha + \sum_{i=1}^{m}Y_i - 1}e^{-(\beta + m) \mu}\lambda^{\nu + \sum_{j=m+1}^{n}Y_j - 1}e^{-(\phi + n - m)\lambda}}
{\sum_{k = 1}^{n - 1} c \,  \mu^{\alpha + \sum_{i=1}^{k}Y_i - 1}e^{-(\beta + k) \mu}\lambda^{\nu + \sum_{j=k+1}^{n}Y_j - 1}e^{-(\phi + n - k)\lambda}}
\end{align}

Once we cancel out the $c$, this serves as our discrete posterior distribution on $m$.


---
# IV. Conditional Posterior for $\mu$

We start by finding the normalizing constant that we'll use in the denominator, $f(\lambda, m \,|\, Y_\mu, Y_\lambda)$.

\begin{align}
&= \int_0^\infty f(\mu, \lambda, m \,|\, Y_\mu, Y_\lambda) \, \textrm{d}\mu \\
&= \int_{0}^{\infty}c \,  \mu^{\alpha + \sum_{i=1}^{m}Y_i - 1}e^{-(\beta + m) \mu}\lambda^{\nu + \sum_{j=m+1}^{n}Y_j - 1}e^{-(\phi + n - m)\lambda} \,  \mathrm{d} \mu \\
&= c  \, \lambda^{\nu + \sum_{j=m+1}^{n}Y_j - 1}e^{-(\phi + n - m)\lambda}
\int_{0}^{\infty} \mu^{\alpha + \sum_{i=1}^{m}Y_i + 1} e^{-(\beta + m)\mu} \mathrm{d} \mu \\
&= c  \, \lambda^{\nu + \sum_{j=m+1}^{n}Y_j - 1}e^{-(\phi + n - m)\lambda}
\frac{\Gamma(\alpha + \sum_{i=1}^{m}Y_i)}{(\beta + m)^{\alpha + \sum_{i=1}^{m}Y_i}}
\int_{0}^{\infty} \frac{(\beta + m)^{\alpha + \sum_{i=1}^{m}Y_i}}{\Gamma(\alpha + \sum_{i=1}^{m}Y_i)}
\mu^{\alpha + \sum_{i=1}^{m}Y_i + 1} e^{-(\beta + m)\mu} \mathrm{d} \mu \\
&= c  \, \lambda^{\nu + \sum_{j=m+1}^{n}Y_j - 1}e^{-(\phi + n - m)\lambda}
\frac{\Gamma(\alpha + \sum_{i=1}^{m}Y_i)}{(\beta + m)^{\alpha + \sum_{i=1}^{m}Y_i}}\\
\end{align}

---
# IV. Conditional Posterior for $\mu$, cont.

With this normalizing constant in hand, we can write out the form of the posterior for $\mu$, $f(\mu \,|\, \lambda, m, Y_\mu, Y_\lambda)$:

\begin{align}
&= \frac{f(\mu, \lambda, m, \,|\, Y_\mu, Y_\lambda)}{f(\lambda, m \,|\, Y_\mu, Y_\lambda)} \\
&= \frac{c \,  \mu^{\alpha + \sum_{i=1}^{m}Y_i - 1}e^{-(\beta + m) \mu}\lambda^{\nu + \sum_{j=m+1}^{n}Y_j - 1}e^{-(\phi + n - m)\lambda}}{c  \, \lambda^{\nu + \sum_{j=m+1}^{n}Y_j - 1}e^{-(\phi + n - m)\lambda}
\frac{\Gamma(\alpha + \sum_{i=1}^{m}Y_i)}{(\beta + m)^{\alpha + \sum_{i=1}^{m}Y_i}}} \\
&= \frac{(\beta + m)^{\alpha + \sum_{i=1}^{m}Y_i}}{\Gamma(\alpha + \sum_{i=1}^{m}Y_i)}
\mu^{\alpha + \sum_{i=1}^{m}Y_i - 1}e^{-(\beta + m) \mu}
\end{align}

Which is recognizable as the pdf of a Gamma random variable. Therefore,

$$\mu \,|\, m, Y_\mu \sim \textrm{Gamma}(\alpha + \sum_{i=1}^{m}Y_i, \beta + m)$$

---
# IV. Posterior for $\lambda$

Same story as for $\mu$.

$$\lambda \,|\, m, Y_\lambda \sim \textrm{Gamma}(\nu + \sum_{i=m+1}^n Y_i, \phi + n - m)$$

---
# V. Gibbs Sampler

Form a Markov chain that begins by initializing a value for $m_{j-1}$, then iterates through the following three steps many times.

1. Sample $\mu_j$ from $\textrm{Gamma}(\alpha + \sum_{i=1}^{m_{j-1}}Y_i, \beta + m_{i-j})$
2. Sample $\lambda_j$ from $\textrm{Gamma}(\nu + \sum_{i=m_{j-1}+1}^n Y_i, \phi + n - m_{j-1})$
3. Sample $m_j$ from $f(m \,|\, \mu_{j}, \lambda_j, Y_{\mu_{j}}, Y_{\lambda_{j}})$

Our specific scenario: $n = 60$, $\alpha = 10$, $\beta = 4$, $\nu = 8$, and $\phi = 2$.

---

```{r}
it <- 50000
post_samples <- matrix(rep(NA, it * 3), ncol = 3)
colnames(post_samples) <- c("mu", "lambda", "m")
m_j <- 2 # initialize m
for (j in 1:it) {
  # sample mu
  mu_j      <- rgamma(1, alpha + sum(y[1:m_j]), beta + m_j)
  # sample lambda
  lambda_j  <- rgamma(1, nu + sum(y[(m_j+1):n]), phi + (n - m_j))
  # sample m
  m_vec <- rep(NA, n - 1)
  for (k in 1:(n - 1)) {
    m_vec[k] <-  mu_j^(alpha + sum(y[1:k]) - 1) *
      exp(-(beta + k) * mu_j) *
      lambda_j^(nu + sum(y[(k+1):n]) - 1) *
      exp(-(phi + n - k) * lambda_j)
  }
  p <- m_vec/sum(m_vec)
  m_j <- sample(1:(n - 1), size = 1, prob = p)
  # store results
  post_samples[j, "mu"]     <- mu_j
  post_samples[j, "lambda"] <- lambda_j
  post_samples[j, "m"]      <- m_j
}
```

---
# V. Convergence

The first 500 $m_j$:

```{r echo = FALSE, fig.height=3}
df <- data.frame(post_samples) %>%
  mutate(index = 1:it)
ggplot(slice(df, 1:500), aes(x = index, y = m)) +
  geom_line() +
  theme_bw()
```

---
# V. Convergence, cont.

All 50,000:

```{r echo = FALSE, fig.height=6}
p1 <- ggplot(df, aes(x = index, y = m)) +
  geom_line() +
  labs(x = "") +
  theme_bw()
p2 <- ggplot(df, aes(x = index, y = mu)) +
  geom_line() +
  labs(y = expression(mu), x = "") +
  theme_bw()
p3 <- ggplot(df, aes(x = index, y = lambda)) +
  geom_line() +
  labs(y = expression(lambda)) +
  theme_bw()
grid.arrange(p1, p2, p3, ncol = 1)
```

---
# V. Thinning

To investigate serial correlation, we plot each sampled parameter against its subsequent draw.

```{r echo = FALSE, fig.height = 3.5, fig.width = 10}
# m serial dependence
df_m <- data.frame(theta        = df$m[1:(nrow(df) - 1)],
                   theta_plus_1 = df$m[2:nrow(df)])
p_m <- ggplot(df_m, aes(x = theta, y = theta_plus_1)) +
  geom_point(alpha = .05) +
  labs(x = expression(m^{i}),
       y = expression(m^{i+1})) +
  theme_bw()

# mu serial dependence
df_mu <- data.frame(theta        = df$mu[1:(nrow(df) - 1)],
                    theta_plus_1 = df$mu[2:nrow(df)])
p_mu <- ggplot(df_mu, aes(x = theta, y = theta_plus_1)) +
  geom_point(alpha = .05) +
  labs(x = expression(mu^{i}),
       y = expression(mu^{i+1})) +
  theme_bw()

# lambda serial dependence
df_lambda <- data.frame(theta        = df$lambda[1:(nrow(df) - 1)],
                        theta_plus_1 = df$lambda[2:nrow(df)])
p_lambda <- ggplot(df_lambda, aes(x = theta, y = theta_plus_1)) +
  geom_point(alpha = .05) +
  labs(x = expression(lambda^{i}),
       y = expression(lambda^{i+1})) +
  theme_bw()

grid.arrange(p_m, p_mu, p_lambda, ncol = 3)
```

Correlations: `r round(cor(df_m)[1,2], 3)`, `r round(cor(df_mu)[1,2], 3)`, and `r round(cor(df_lambda)[1,2], 3)`.

---

# V. Thinning

We can remove the Markov dependence by retaining only every tenth sample.

```{r}
df <- df %>%
  slice(seq(from = 1, to = nrow(df), by = 10))
```


```{r echo = FALSE, fig.height = 3.5, fig.width = 10}
# m serial dependence
df_m <- data.frame(theta        = df$m[1:(nrow(df) - 1)],
                   theta_plus_1 = df$m[2:nrow(df)])
p_m <- ggplot(df_m, aes(x = theta, y = theta_plus_1)) +
  geom_point(alpha = .05) +
  labs(x = expression(m^{i}),
       y = expression(m^{i+1})) +
  theme_bw()

# mu serial dependence
df_mu <- data.frame(theta        = df$mu[1:(nrow(df) - 1)],
                    theta_plus_1 = df$mu[2:nrow(df)])
p_mu <- ggplot(df_mu, aes(x = theta, y = theta_plus_1)) +
  geom_point(alpha = .05) +
  labs(x = expression(mu^{i}),
       y = expression(mu^{i+1})) +
  theme_bw()

# lambda serial dependence
df_lambda <- data.frame(theta        = df$lambda[1:(nrow(df) - 1)],
                        theta_plus_1 = df$lambda[2:nrow(df)])
p_lambda <- ggplot(df_lambda, aes(x = theta, y = theta_plus_1)) +
  geom_point(alpha = .05) +
  labs(x = expression(lambda^{i}),
       y = expression(lambda^{i+1})) +
  theme_bw()

grid.arrange(p_m, p_mu, p_lambda, ncol = 3)
```

Correlation: `r round(cor(df_m)[1,2], 3)`, `r round(cor(df_mu)[1,2], 3)`, and `r round(cor(df_lambda)[1,2], 3)`.

---
# VI. Conclusions

What is our updated best guess for when the changepoint occurred? Consider the (thinned) posterior distribution on $m$.

```{r echo = FALSE, fig.height = 3.5, fig.width = 8}
ggplot(df, aes(x = m)) +
  geom_bar() +
  labs(x = "m", y = "") +
  theme_bw() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  scale_x_continuous(breaks = c(1, 10, 20, 30, 40, 50, 60))
```

Those two prominent modes are at 34 and 37, so those are probably fine point estimates for $m$ based on this data and your prior information.

---
# VI. Conclusions, cont.

Consider our this final posterior changes if we use a different seed / get a different data set $Y$.

```{r echo = FALSE, fig.height = 3.5, fig.width = 8}
set.seed(497)
y_mu <- rpois(m, lambda = mu)
y_lambda <- rpois(n - m, lambda = lambda)
y <- c(y_mu, y_lambda)
alpha <- 10
beta <- 4
nu <- 8
phi <- 2
it <- 50000
post_samples <- matrix(rep(NA, it * 3), ncol = 3)
colnames(post_samples) <- c("mu", "lambda", "m")
m_j <- 2 # initialize m
for (j in 1:it) {
  # sample mu
  mu_j      <- rgamma(1, alpha + sum(y[1:m_j]), beta + m_j)
  # sample lambda
  lambda_j  <- rgamma(1, nu + sum(y[(m_j+1):n]), phi + (n - m_j))
  # sample m
  m_vec <- rep(NA, n - 1)
  for (k in 1:(n - 1)) {
    m_vec[k] <-  mu_j^(alpha + sum(y[1:k]) - 1) *
      exp(-(beta + k) * mu_j) *
      lambda_j^(nu + sum(y[(k+1):n]) - 1) *
      exp(-(phi + n - k) * lambda_j)
  }
  p <- m_vec/sum(m_vec)
  m_j <- sample(1:(n - 1), size = 1, prob = p)
  # store results
  post_samples[j, "mu"]     <- mu_j
  post_samples[j, "lambda"] <- lambda_j
  post_samples[j, "m"]      <- m_j
}
df <- data.frame(post_samples) %>%
  mutate(index = 1:it) %>%
  slice(seq(from = 1, to = nrow(df), by = 10))
ggplot(df, aes(x = m)) +
  geom_bar() +
  labs(x = "m", y = "") +
  theme_bw() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  scale_x_continuous(breaks = c(1, 10, 20, 30, 40, 50, 60))
```

With a flat prior on $m$, the posterior will be drawing most of its structure from the data, which, at these sample sizes, are still subject to considerable sampling variability.
